{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354deb0-49c2-43fd-8c5f-a07f2e07b8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install labelme tensorflow opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20bfca-141f-4260-94c6-210cea502418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cccae-5b40-464b-a77a-ae7026b74de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952ddd5-3041-4c99-8f9b-1968ca5c9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac60d3-63c1-4e31-bb4e-de597a2f4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join('data', 'images')\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0a90e-5ff8-4cda-811d-3a6359a1ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e127400-4dc8-42cd-8505-625b431e429d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print('capturing image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(img_path, f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    #cv2.imshow('frame', frame)\n",
    "    time.sleep(1)\n",
    "\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "#cap.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc32a6b-3d16-4b16-b72c-ed278d57b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2dbb91-5f73-441f-8881-fc5cf682b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d70a-3ae8-43c0-8505-4c0685639054",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ea8d1-9a5c-4e91-b4c4-d87f7ad3bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3421ea7-e4a3-40ce-9b54-6ddf95283e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg', shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2992ce4-8566-4134-ade2-566f187f8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1754791-fe70-4514-85df-12d8501841a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cd930-3dfd-4fa7-89d8-9806bd5bf39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ed126-6c02-48b9-aeb3-429ed24b9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bfdfb-c4f4-442d-af3f-9987cff5f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f4fce-0cbd-48c3-b7de-493c92456792",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04c80f-1709-4eab-897b-a375deafadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dad0bd-f062-4d7b-89aa-23b2aa882387",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 4, figsize = (20, 20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2834b63-402c-47df-8f8a-ed6329a55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign 63 to train\n",
    "#assign 14 to test\n",
    "#assign 13 to val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5dbd5-1171-4be8-a365-f4acb711a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331214b7-b98c-439b-9b27-d22cdcf381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc74b4-ddc7-4784-a68b-a8e0979db31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc103c-02d3-4c3d-aa68-eb20fe279d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'train', 'images', '55d783c0-4d9e-11ef-b40f-6479f0482dc5.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aba251-45ff-4a51-9f20-e921df9aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '55d783c0-4d9e-11ef-b40f-6479f0482dc5.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066135e-4378-4b29-9e40-31b1219da0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab441421-eeba-4e88-b761-4d2d4cf0ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cords = [0, 0, 0, 0,]\n",
    "cords[0] = label['shapes'][0]['points'][0][0]\n",
    "cords[1] = label['shapes'][0]['points'][0][1]\n",
    "cords[2] = label['shapes'][0]['points'][1][0]\n",
    "cords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117a6f0-93b4-4f0d-b1fe-b544a493c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201a5ac-17cc-4a30-9763-ed19030ad979",
   "metadata": {},
   "outputs": [],
   "source": [
    "cords = list(np.divide(cords, [640, 480, 640, 480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3440ba3-0ca7-4ad1-9d7f-12f12eac6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb6b76-d486-4184-ae12-85e0a283ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image = img, bboxes = [cords], class_labels = ['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c114d11-1eae-4562-8286-6fd1ba3ebf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfabdf7-ee03-47e1-979b-d0220d9e00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ba64a-2528-4dea-ab6d-a4b0fa32db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_img)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cee138-270e-4792-b7e9-3262ca672f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_img)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c953a64-785b-4a95-9b69-c581b873f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_img)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbcea38-d543-4976-8b1d-6301cf4a8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bcf71-ad5c-46f6-8751-25876b0a1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea39e6-9e0c-44d2-805e-c18b01eadffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe43d5-5567-460c-b5b7-f393fda43041",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e67aea-c9d6-42e2-aebe-f24c494057c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5842a-2928-4017-baf2-105a03bbf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efb8b1-c42f-4c17-bf23-033bb6f2e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(val_images), len(val_labels), len(test_images), len(test_labels), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae01f3-71d3-433c-b09c-3fdd645b10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbf659-7e75-49a4-a2ed-c0026d64b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7adf42-72eb-4e78-b4ee-6eb06e4ba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0588959-bbbc-4c3e-b11c-9788bdcd7dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246815e-d423-48e6-ade9-0cac5b09ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7c8b7-d497-432a-9d35-7c602fb38d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ff9d8-08a2-440e-b4e1-3ae555ec30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    sample_image = sample_image.copy()\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10076e9-d6c4-40fc-b897-7ccd5ddd1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ffbc4-dd4c-4886-aefc-6edc5d21952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde320f-a9b5-43fe-9d23-749b8dd27e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd19a9-8d71-49ee-9c97-7eff63e3b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape = (120, 120, 3))\n",
    "\n",
    "    vgg = VGG16(include_top = False)(input_layer)\n",
    "    #classification miodel\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation = 'relu')(f1)\n",
    "    class2 = Dense(1, activation = 'sigmoid')(class1)\n",
    "    #bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation = 'relu')(f2)\n",
    "    regress2 = Dense(4, activation = 'sigmoid')(regress1)\n",
    "\n",
    "\n",
    "    facetracker = Model(inputs = input_layer, outputs = [class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92845385-a2cc-4aff-9d7e-d63d781eddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002e73-2fec-491d-879d-676a4a3c8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d125b-c77d-4566-a3c9-407e10f65810",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299d896-0edb-4a80-93c2-e6ce6d0356f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9a5c8-697a-4f50-bae0-92a8be616340",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, cords = facetracker.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371aa957-1815-4a19-8cd7-c423e59028ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98787216-12ed-4bc8-a553-0004a5e6518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ac6b7-2dd2-47ec-9cc2-10e3308d7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1/0.75 - 1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb47d83-cd5d-4ac8-a911-4a44c14da5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71422ed-ae64-4cd8-b9bf-fa8f2bf40d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240829db-0252-4bcb-b0bc-c786cd08be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46e268-efdc-47a3-9be9-eefa614bde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100d450-b25a-4114-ad2b-9e47903d3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], cords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c468845-d7cf-485f-81b4-5c3dbb2e05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2e76a-7a11-4cbc-953c-5b9cee08529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], cords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c90ce1-1b84-4e01-a7c7-a87629716e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, facetracker,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        y[0].set_shape([None,1])\n",
    "\n",
    "        # Print shapes for debugging\n",
    "        # print(f'X shape: {X.shape}')\n",
    "        # print(f'y[0] shape (classes): {y[0].shape}')\n",
    "        # print(f'y[1] shape (coords): {y[1].shape}')\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            # print(f'Predicted classes shape: {classes.shape}')\n",
    "            # print(f'Predicted coords shape: {coords.shape}')\n",
    "\n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "\n",
    "            total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "\n",
    "        self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        y[0].set_shape([None,1])\n",
    "\n",
    "        classes, coords = self.model(X, training=False)\n",
    "\n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f953ac5-dbea-4543-8ca5-46a3a193ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29c6af-2b2e-45bb-97e4-b54b8ff7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa56cb9-c018-4c14-83af-174a190f519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e893c10-a8db-48ef-8d7a-3d49bec02344",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba79371-c9f4-4bb5-ae90-2c43bb75df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=5, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cf9a6-04b5-48fc-a512-4acfff514e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0227a6-2739-4be3-971b-2903114366d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80452b-4cbb-4099-a42c-fc1e9afb00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc396764-55dc-4b32-a247-935207c25826",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfb85c-815d-40c8-9304-3e88e01c9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    sample_image = sample_image.copy()\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239be76-aceb-48e9-8948-d94c3bfd42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747749d-d8b2-453a-98fa-e25efb29517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b45e50-53b8-4af0-9810-7b6bdc968887",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596cf67-b365-4089-b187-9300ddae212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d83c1-fb4a-4564-b0fa-469f50545ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.getBuildInformation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071542bd-6a52-4455-b545-975fd0a6893d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Use the default camera\n",
    "plt.ion()  # Turn on interactive mode for matplotlib\n",
    "\n",
    "while cap.isOpened():\n",
    "    _, frame = cap.read()\n",
    "    frame = frame[50:500, 50:500, :]\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120, 120))\n",
    "\n",
    "    yhat = facetracker.predict(np.expand_dims(resized / 255, 0))\n",
    "    sample_coords = yhat[1][0]\n",
    "\n",
    "    if yhat[0] > 0.5:\n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame,\n",
    "                      tuple(np.multiply(sample_coords[:2], [450, 450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450, 450]).astype(int)),\n",
    "                      (255, 0, 0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame,\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [0, -30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [80, 0])),\n",
    "                      (255, 0, 0), -1)\n",
    "\n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                                [0, -5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame using matplotlib\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)  # Pause for a short interval to update the plot\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if plt.waitforbuttonpress(timeout=0.1) and plt.get_current_fig_manager().canvas.key_press_handler_id == 'q':\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()  # Ensure the last frame is displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236c9c2-f0a4-4cdf-ab1c-7175ad6c85d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceDetection",
   "language": "python",
   "name": "facedetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
